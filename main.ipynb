{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminararbeit: Machine Learned Hyperelastic Potentials\n",
    "Moritz Trieu [3532051]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The issue at hand\n",
    "Every material (in this case hyperelastic materials) has an potential energy function. This energy functions is affected by the strain tensor and therefor lends itself to approximate the real-world energy function via deep learning. The strain tensor, as a symmetrical tensor, has 6 dregrees of freedom. But of course every degree is prone to measuring inaccuracies, so it might be that 2 strain tensors from the same material have slightly different values and are then approximated with differen values. \n",
    "A higher discrepancies of values might occur when measuring the same materiel from 2 different angles and/ or perspectives. In that case we would two rotated tensor, that mathematicially describe the same phenomenon from a different point of view. But as a neural network is unware of this fact, it might also approximate these two tensors with different values. \n",
    "To avoid this problem, we look to train the network with the tensor invariants - which are invariant under transformation - and/ or the eigenvalues of the tensor, which describe the main axis of strain of a given material. This in theory could give us a much better approximation as we are less prone to inaccuracies of the measured tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Questions\n",
    "- Do we need a different model for every material?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the conda environment\n",
    "Python version: 3.10.4\n",
    "\n",
    "Libraries:\n",
    "- Pytorch\n",
    "- Matplotlib\n",
    "- Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all neccassary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for GPU avalibility and set global device status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available on this system, setting device to GPU\n"
     ]
    }
   ],
   "source": [
    "system_device =  \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    print(f\"CUDA is available on this system, setting device to GPU\")\n",
    "    system_device = \"cuda\"\n",
    "else:\n",
    "    print(f\"CUDA not available on this system, setting device to CPU\")\n",
    "\n",
    "device = torch.device(system_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing General Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I create the overall template for every neural network I will be using in this project. As I want to compare the impact of different types inputs (for now the difference between the strain tensor and the strain tensor invariants, later on maybe the eigen-values of the strain tensor and any combination of those) the only difference between the networks should be the input size. The rest of the architecture stays the same and is a baseline architechture of 3 layers with a Leaky RelU activation function. This seems to be the best option for now as we aren't computing propabilities and Leaky RelU also combats the dead neuron issue.\n",
    "\n",
    "## Comparison of different activation functions\n",
    "Originally I wanted to use LeakyRelU as the default choice for activation functions. But as we need the derivation of the network function, the activation function needs to be differentiable. The RelU function is not differentiable, so there is the need to look into alternatives. \n",
    "### GelU\n",
    "Gaussian Error Linear Unit is defined as GELU(x) = x * \\Psi(x) where \\Psi(x) is the standard Gaussian cumulative distribution function. The function can be approximated with GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/\\pi) * (0.044715 x ** 3))) or x * \\sigma(1.702 * x)\n",
    "Implemented in PyTorch. As it can be seen as a smoother RelU function, it is reasonable to assume that the function has similar shortcomings to RelU, mainly the dead neuron issue, even if not as pronounced. If we only work with smaller values between -2 and 2, this might not be a problem.\n",
    "\n",
    "Sources: \n",
    "- https://paperswithcode.com/method/gelu\n",
    "- https://arxiv.org/abs/1606.08415v4\n",
    "- https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.48.44_PM.png\n",
    "\n",
    "### Softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Hyperparameteres\n",
    "hidden_size = 8\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "class GeneralNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=hidden_size):\n",
    "        super(GeneralNeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.l1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.l2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.l3 = nn.Linear(self.hidden_size, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.leaky_relu(self.l1(input))\n",
    "        out = self.leaky_relu(self.l2(out))\n",
    "        out = self.l3(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up STRAIN model\n",
    "This is our baseline model with 6 degrees of freedom. The inputs are the values of the strain tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_input_size = 6\n",
    "\n",
    "strain_model = GeneralNeuralNet(strain_input_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up INVARIANT model\n",
    "This is the model trained with the invariants of the tensors. The invariants are calculated with the measured tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "invariant_input_size = 3\n",
    "\n",
    "invariant_model = GeneralNeuralNet(invariant_input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimizer, Scheduler\n",
    "Note: I'm generating 2 different Loss functions here. As far as I understand, one would be sufficient. In every training loop the criterion is passed a torch.tensor with the predicted value and the tensor contains the grad history of the network, the training loop of every network should get it's own computation graph by autograd, regardless of which criterion is used or was used before. I just don't want to take any risks and have potentials bugs because of this. So having 2 different criterions is just a safety measure and might get replaced later if I see, that it will make no difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Loss Function\n",
    "strain_criterion = torch.nn.MSELoss() # TODO: Using MSE Loss for now, if the time is there implement a custom loss function\n",
    "invariant_criterion = torch.nn.MSELoss()\n",
    "\n",
    "#Optimizer\n",
    "strain_optim = torch.optim.SGD(strain_model.parameters(), lr=learning_rate)\n",
    "invariant_optim = torch.optim.SGD(invariant_model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Scheduler\n",
    "strain_scheduler = torch.optim.lr_scheduler.StepLR(strain_optim, step_size=30, gamma=0.1)\n",
    "invariant_scheduler = torch.optim.lr_scheduler.StepLR(invariant_optim, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train strain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save start time for training loop\n",
    "strain_model_start_time = perf_counter()\n",
    "\n",
    "for epoch in len(num_epochs):\n",
    "    # Empty out grad\n",
    "    strain_optim.zero_grad()\n",
    "\n",
    "    # Convert Data\n",
    "\n",
    "    # Forward Pass\n",
    "    prediction = strain_optim(data) # TODO: Ingest actual data\n",
    "    loss = strain_criterion(prediction, actual_values) # TODO: Ingest actual data\n",
    "\n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "    strain_optim.step()\n",
    "\n",
    "    # Learning Rate Step\n",
    "    strain_scheduler.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"STRAIN: Epoch: {epoch+1}/{num_epochs}: Loss: {loss.item():.4f} Learning Rate: {strain_scheduler.get_last_lr()} Time Passed: {(perf_counter() - strain_model_start_time):.4f}\")\n",
    "\n",
    "strain_model_end_time = perf_counter()\n",
    "strain_model_time_delta = strain_model_end_time - strain_model_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Invariant Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save start time for training loop\n",
    "invariant_model_start_time = perf_counter()\n",
    "\n",
    "for epoch in len(num_epochs):\n",
    "    # Empty out grad\n",
    "    invariant_optim.zero_grad()\n",
    "\n",
    "    # Convert Data\n",
    "\n",
    "    # Forward Pass\n",
    "    prediction = invariant_optim(data) # TODO: Ingest actual data\n",
    "    loss = invariant_criterion(prediction, actual_values) # TODO: Ingest actual data\n",
    "\n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "    invariant_optim.step()\n",
    "\n",
    "    # Learning Rate Step\n",
    "    invariant_scheduler.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"INVARIANT Epoch: {epoch+1}/{num_epochs}: Loss: {loss.item():.4f} Learning Rate: {strain_scheduler.get_last_lr()} Time Passed: {(perf_counter() - strain_model_start_time):.4f}\")\n",
    "\n",
    "invariant_model_end_time = perf_counter()\n",
    "invariant_model_time_delta = invariant_model_end_time - invariant_model_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracies of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08947ae1826f2ece435e966b892878742bad16fb920615abadf254de0d5b50ce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jupyter-seminar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
